import copy
import json
import os

task_type_prompt = "Task type: time series classfication"


list_templates_by_chatgpt = [
    """<query>
以下时间序列是由EPANET生成的，EPANET是一个对配水管道系统的水力和水质行为进行建模的软件应用程序。
EPANET可以在给定的供水网络中进行跟踪模拟持续时间内每个水箱中的水位和压力、管道中的水流量以及整个网络中化学物质（在本例中为氯）的浓度。
每个时间序列都是166个节点处氯浓度水平的测量值（管道连接处）。在15天内每5分钟进行一次测量。
给定时间序列：<ts-data-blank-sep>，其中数据点以空格分隔。判断其所属的类别。
<options>
可能的类别包括：1,2,3。
请从上述选项中选择一个回答：
<response>
""",
     """<query>
以下时间序列是由EPANET生成的，EPANET是一个对配水管道系统的水力和水质行为进行建模的软件应用程序。
EPANET可以在给定的供水网络中进行跟踪模拟持续时间内每个水箱中的水位和压力、管道中的水流量以及整个网络中化学物质（在本例中为氯）的浓度。
每个时间序列都是166个节点处氯浓度水平的测量值（管道连接处）。在15天内每5分钟进行一次测量。
当前的时间序列为：<ts-data-comma-sep>，其中数据点以逗号分隔。请判断其所属的类别：
<options>
可能的类别包括：1,2,3。
请从上述选项中选择一个回答。
<response>
""",
    """<query>
给定时间序列：<ts-data-blank-sep>，其中数据点以空格分隔。
该时间序列是由EPANET生成的，EPANET是一个对配水管道系统的水力和水质行为进行建模的软件应用程序。
EPANET可以在给定的供水网络中进行跟踪模拟持续时间内每个水箱中的水位和压力、管道中的水流量以及整个网络中化学物质（在本例中为氯）的浓度。
每个时间序列都是166个节点处氯浓度水平的测量值（管道连接处）。在15天内每5分钟进行一次测量。
请判断所给的时间序列的类别：
<options>
可能的类别包括：1,2,3。
请从上述选项中选择一个回答。
<response>
""",
    """<query>
The following time series were generated by EPANET, a software application that models the hydraulic and water quality behavior of water distribution piping systems.
EPANET can track water levels and pressures in each tank, water flow in pipes, and concentrations of chemicals (in this case chlorine) throughout a given water network over the duration of a simulation.
Each time series is a measurement of chlorine concentration levels at 166 nodes (pipe connections). Measurements were taken every 5 minutes for 15 days.
Given a time series: <ts-data-blank-sep> where data points are separated by spaces. Determine the category it belongs to.
<options>
Possible categories include: 1,2,3.
Please select an answer from the options above:
<response>
""",
    """<query>
The following time series were generated by EPANET, a software application that models the hydraulic and water quality behavior of water distribution piping systems.
EPANET can track water levels and pressures in each tank, water flow in pipes, and concentrations of chemicals (in this case chlorine) throughout a given water network over the duration of a simulation.
Each time series is a measurement of chlorine concentration levels at 166 nodes (pipe connections). Measurements were taken every 5 minutes for 15 days.
The current time series is: <ts-data-comma-sep>, where data points are separated by commas. Please determine the category it belongs to:
<options>
Possible categories include: 1,2,3.
Please select an answer from the options above.
<response>
""",
    """<query>
Given a time series: <ts-data-blank-sep> where data points are separated by spaces.
The time series was generated by EPANET, a software application that models the hydraulic and water quality behavior of water distribution piping systems.
EPANET can track water levels and pressures in each tank, water flow in pipes, and concentrations of chemicals (in this case chlorine) throughout a given water network over the duration of a simulation.
Each time series is a measurement of chlorine concentration levels at 166 nodes (pipe connections). Measurements were taken every 5 minutes for 15 days.
Please determine the category of the given time series:
<options>
Possible categories include: 1,2,3.
Please select an answer from the options above.
<response>
"""
]


label_map = {
    "1": "1",
    "2": "2",
    "3": "3",
}

label_map_zh = {
    "1": "1",
    "2": "2",
    "3": "3",
}


list_templates = []
list_templates.extend(list_templates_by_chatgpt)

task_name = "ChlorineConcentration"
to_folder = os.path.join(
    "UCRArchive_2018/prompt_datasets/", f"{task_name}"
)
os.makedirs(to_folder, exist_ok=True)

for mode in ["TRAIN", "TEST"]:
    list_datas = []
    with open(f"UCRArchive_2018/{task_name}/{task_name}_{mode}.tsv", "r", encoding="utf-8") as f:
        for row in f:
            row = row.strip()
            if not row:
                continue

            label_ = row.split("\t")[0]
            label_name_ = label_map[label_]
            label_zh_name_ = label_map_zh[label_]

            ts_data_ = row.split("\t")[1: ]
            # assert len(ts_data_) == 96
            ts_data_blank_sep = " ".join(ts_data_)
            ts_data_comma_sep = ",".join(ts_data_)

            for template_ in list_templates:
                template_ = copy.copy(template_)

                # 确认数据怎么分割的
                if "comma-sep" in template_:
                    template_ = template_.replace(
                        "<ts-data-comma-sep>",
                        ts_data_comma_sep
                    )
                else:
                    template_ = template_.replace(
                        "<ts-data-blank-sep>",
                        ts_data_blank_sep
                    )

                # 确认中文还是英文
                target = ""
                if "序列" in template_:
                    target = label_zh_name_
                else:
                    target = label_name_

                list_datas.append(
                    {
                        "original_data": (label_name_, ts_data_blank_sep),
                        "target": target,
                        "input": template_,
                        "task_name": task_name,
                        "task_type": "classification",
                    }
                )

    print(f"len(list_datas): {len(list_datas)}")

    with open(os.path.join(to_folder, f"{mode}.json"), "w", encoding="utf-8") as f:
        for samp in list_datas:
            f.write(
                json.dumps(samp, ensure_ascii=False) + "\n"
            )


    